---
title: "USF Marketing Site Web Traffic Analysis"
output: html_document
---

<link rel="stylesheet" type="text/css" href="./style/style.css">

This project has several parts:

<ol>
<li>Fetch data updates using Google drive API</li>
<li>Read and Write from a database</li>
<li>Automated analysis of several time series models: ARMA, VARMA, VARIMA, SARIMA</li>
<li>Run this software on a Raspberry Pi</li>
</ol>

```{r setup, include=FALSE}
library(googledrive)
library(RMySQL)
library(reshape2)
library(magrittr)
library(vars)
library(MTS)
library(gplots)
library(beepr)
library(gridExtra)

output.plots <- FALSE
```


#### 1. Connect to Google Drive
```{r googleDrive}
data.folder.id <- "1CakyziIpT39meYk05QaBNNEQf4aoWtXh"

#if you don't have permission, stop the script
tryCatch({
  data.folder <- drive_get(as_id(data.folder.id))
},error=function(e){
  print(e)
  drive_deauth()
  stop()
})

file.tibble <- drive_ls(data.folder)


for (key in file.tibble$id){
  tryCatch({
    setwd("./data")
    drive_download(as_id(key)) #download files to localhost
  },error=function(e){
    #do nothing
  })
}

#read files from localhost

data <- read.csv(file.tibble[1,"name"] %>% as.character())
if (nrow(file.tibble) > 1){
  for (filename in file.tibble[2:nrow(file.tibble),"name"] %>% unlist()) {
    data <- rbind(data,read.csv(filename)) #concatenate files
  }
}
names(data) <- c("Page","Date","Pageviews","Unique Pageviews","Avg Time on Page","Entrances","Bounce Rate","Percent Exit","Page Value")
data <- as.data.frame(data)
```

#### 2. Import data to database
```{r intoDatabase}
database <- dbConnect(MySQL(), user='root', password='MyNewPass', dbname='senior_project', host='localhost')

dbWriteTable(database,name="webtraf",data,append=TRUE,row.names=FALSE)
```

```{r fetchFromDatabase, include=FALSE}
dbSendQuery(database, 'SELECT * FROM webtraf;') #any SQL statement works here!

request <- dbListResults(database)[[1]]

#n = -1 asks the server to send all pending requests
web.traffic.data <- fetch(request,n=-1)
web.traffic.data <- web.traffic.data[!duplicated(web.traffic.data),]
```

## Analysis

#### SSE Heatmap of ARMA(p,q) on 4 out of 10 Highest Activity Pages
```{r fitARMA, include=FALSE}
#will use string of page url for index
unique.pages <- web.traffic.data$Page %>% unique()
unique.pages <- unique.pages[!unique.pages==""]
  
tryCatch({
  setwd("./rds")
   fit.armas.grid.search <- readRDS("fitARMAsGridSearch.rds")
},error=function(e){
  
  
  
  fit.armas.grid.search <- list()
  
  #compute SSE for p=1:14 x q=1:14
  startTime <- Sys.time()
  for (i in 1:length(unique.pages)) {
    time.series <- web.traffic.data[which(web.traffic.data$Page == unique.pages[[i]]),"Pageviews"]
    train <- time.series[1:635]
    
    fit.armas <- list()
    for (p in 0:14) {
      for (q in 0:14) {
        tryCatch({
          fit.armas[[14*p + q + 1]] <- arima(train,c(p,0,q))
        }, error = function(e){
          fit.armas[[14*p + q + 1]] <- NULL
        })
        
        percentComplete <- ((14*p + q)*i/(14*14*10))
        runningTime <- Sys.time() - startTime
        estimateOfTotalTime <- runningTime/percentComplete
        
        estimatedFinishTime <- startTime + estimateOfTotalTime
        
        #progress
        print(paste("Start:",startTime,"Est. duration:",estimateOfTotalTime,"Est. finish time:",estimatedFinishTime,i,p,q,sep=" "))
      }
    }
    
    fit.armas.grid.search[[i]] <- fit.armas
    rm(fit.armas)
  }
  setwd("./rds")
  saveRDS(fit.armas.grid.search,"fitARMAsGridSearch.rds")
})
```

```{r ARMApredictions, include=FALSE}
time.series <- list()
train.data <- list()
test.data <- list()
for (i in 1:length(unique.pages)) {
  time.series[[i]] <- web.traffic.data[which(web.traffic.data$Page == unique.pages[[i]]),"Pageviews"]
  train.data[[i]] <- time.series[[i]][1:635]
  test.data[[i]] <- time.series[[i]][636:794]
}

arma.preds <- list()
for (i in 1:length(unique.pages)) {
  grid.of.preds <- list()
  for (p in 1:14) {
    for (q in 1:14) {
      tryCatch({
        grid.of.preds[[14*p + q + 1]] <- predict(fit.armas.grid.search[[i]][[14*p + q + 1]],newdata=test.data[[i]],n.ahead=144)
      },error=function(e){
        #print(paste(i,p,q))
      })
      
    }
  }
  arma.preds[[i]] <- grid.of.preds
  rm(grid.of.preds)
}
```

```{r evaluateRootSumSquaredResiduals, include=FALSE}
page.grid.of.rssr <- list()
for (i in 1:length(unique.pages)) {
  rssr <- matrix(nrow=14,ncol=14)
  for (p in 1:14){
    for(q in 1:14) {
      tryCatch({
        if(is.null(arma.preds[[i]][[14*p + q + 1]])) {
          rssr[p,q] <- NA
        }else{
          rssr[p,q] <- arma.preds[[i]][[14*p + q + 1]]$se %>% sum() %>% sqrt()
        }
        
      },error=function(e){
        rssr[p,q] <- NA
      })
        #rssr[p,q] <- .Machine$integer.max
    }
  }
  page.grid.of.rssr[[i]] <- rssr
  rm(rssr)
}

page.min.rssr <- list()
indices.of.optimal <- list()
for (i in 1:length(unique.pages)){
  page.min.rssr[[i]] <- page.grid.of.rssr[[i]][!is.na(page.grid.of.rssr[[i]])] %>% unlist() %>% min()
  indices.of.optimal[[i]] <- which(page.grid.of.rssr[[i]] %>% unlist() == page.min.rssr[[i]])
}

optimal.parameters <- list()
for (i in 1:length(unique.pages)){
  optimal.parameters[[i]] <- c(floor(indices.of.optimal[[1]]/14),indices.of.optimal[[i]] %% 14)
}

setwd("./img/ARMA_SSE_Heatmaps")
heatmaps <- list()
for(i in 1:length(unique.pages)){
  page.grid.of.rssr[[i]][is.na(page.grid.of.rssr[[i]])] <- .Machine$integer.max
  page.grid.of.rssr[[i]] <- page.grid.of.rssr[[i]] - (page.grid.of.rssr[[i]] %>% min())
  page.grid.of.rssr[[i]][page.grid.of.rssr[[i]]==max(page.grid.of.rssr[[i]])] <- (sort(page.grid.of.rssr[[1]]) %>% unique())[[(sort(page.grid.of.rssr[[1]]) %>% unique() %>% length()) - 1]]
  
  png(paste("SSE of ARMA(p,q) on Page ",i,".png",sep=""))
  curr.heatmap <- heatmap.2(page.grid.of.rssr[[i]],Rowv=FALSE,Colv=FALSE,tracecol = NA)
  title("ARMA(p,q) Sum of Squared Residuals")
  text(.55,.82,unique.pages[[i]])
  text(.4,.95,paste("Optimal Model: ARMA(",paste(optimal.parameters[[i]],collapse=","),")",sep=""))
  heatmaps[[i]] <- curr.heatmap
  dev.off()
}
```

<table>
<tr>
  <td>
    <a href="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 1.png" target="_blank"></td><td><img src="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 1.png" /></a>
  </td>
  <td>
    <a href="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 3.png" target="_blank"></td><td><img src="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 3.png" /></a>
  </td>
</tr>
<tr>
  <td>
      <a href="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 7.png" target="_blank"></td><td><img src="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 7.png" /></a>
    </td>
    <td>
      <a href="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 10.png" target="_blank"></td><td><img src="./img/ARMA_SSE_Heatmaps/SSE of ARMA(p,q) on Page 10.png" /></a>
  </td>
</tr>
</table>

TODO LINK TO PAGE OF ALL HEATMAPS

```{r plotPredictions, include=FALSE}
if (output.plots){
  setwd("./img/ARMAPredictionPlots")
  #readline("Would you like to ")
  for (i in 1:length(unique.pages)) {
    for (p in 1:14) {
      for (q in 1:14) {
        
        tryCatch({
          if (is.null(arma.preds[[i]][[14*p + q + 1]]$pred)){
          next
        }
        },error=function(e){
          print(paste(i,p,q))
        })
        
        
        title <- paste("Page_",i,"_ARMA(",p,",",q,").png",sep="")
        png(title)
        plot(1:144,test.data[[i]][1:144])
        title(title)
        lines(1:144,test.data[[i]][1:144])
        tryCatch({
          
          lines(1:144,arma.preds[[i]][[14*p + q + 1]]$pred,col="red")
        },error=function(e){
          print(paste(i,p,q))
          plot.new()
          tryCatch({(
            next
          )},error=function(e){
            #tried to next on last iteration. safe to ignore
          })
        })
        dev.off()
      }
    }
  }
}
```
These 10 ARMA models achieved their peak predicitve ability where p = 9. There was less agreement on the moving average portion, but q = 7 is the only one that came up twice. For modeling these pages in general, I would move forward with ARMA(9,7). The following plots are predictions using ARMA(9,7).

TODO square of 4 predictions
TODO LINK TO ALL PREDICTIONS FOR EACH PAGE

## VAR
```{r var, include=FALSE}

unique.dates <- unique(web.traffic.data$Date) %>% sort()

vector.form.data <- matrix(nrow=length(unique.dates),ncol=length(unique.pages))
for(k in 1:length(unique.dates)) {
  relevant.rows <- web.traffic.data[which(web.traffic.data$Date == unique.dates[[k]]),]
  
  for(i in 1:length(unique.pages)) {
    vector.form.data[k,i] <- relevant.rows[which(relevant.rows$Page == unique.pages[[i]]),"Pageviews"]
  }
  #vector.form.data[k,i+1] <- unique.dates[[k]]
}

vector.form.data <- apply(vector.form.data,2,as.numeric)

opt.var <- VARselect(vector.form.data[,1:10],lag.max = 14)

vector.form.data <-cbind(vector.form.data,unique.dates) %>% as.data.frame()
```

VARSelect 

Since these models intend to be used for prediction, using AIC is desirable. The fact 8 showed up again using another selection criterion affirms our choice slightly. I would move forward with VAR(8).

TODO VAR(8) predictions on the 4 pages
TODO LINK TO THE REST OF THEM

## VARMA
```{r fitvarma, include=FALSE}
varma.train <- vector.form.data[1:780,]
varma.test <- vector.form.data[781:794,]

tryCatch({
  setwd("./rds")
  varma.fit <- readRDS("varmaFit.rds")
},error=function(e){
  varma.fit <- VARMA(vector.form.data,p=1,q=1)
  setwd("./rds")
  saveRDS(varma.fit,"varmaFit.rds")
})
beep(1)
```

```{r testNullHypotheses, include=FALSE}
parameter.estimates <- varma.fit$coef
estimate.ses <- varma.fit$secoef

reject.null.hypotheses <- abs(parameter.estimates) > (estimate.ses * 1.96) #HT param = 0 @ 95% significance
reject.null.hypotheses[is.na(reject.null.hypotheses)] <- FALSE
reject.null.hypotheses[reject.null.hypotheses] <- 1
reject.null.hypotheses[!reject.null.hypotheses] <- 0

not.reject.null.hypotheses <- reject.null.hypotheses
not.reject.null.hypotheses[which(reject.null.hypotheses == 1)] <- 0
not.reject.null.hypotheses[which(reject.null.hypotheses == 0)] <- 1

heatmap.2(not.reject.null.hypotheses,Rowv=FALSE,Colv=FALSE)
title("Statistically Significant VARMA(1,1) Parameters")
```

```{r visualizePredictions, include=FALSE}
if(output.plots){
  varma.pred <- VARMApred(varma.fit,h=14)
  
  for(page.index in 1:(ncol(varma.train)-1)){
    png(paste("varma",page.index,".png",sep=""))
    
    predictions <- varma.pred$pred[,page.index]
    upper.bound <- predictions + varma.pred$se[,page.index]
    lower.bound <- predictions - varma.pred$se[,page.index]
    
    y.plot.upper.bound <- c(varma.test[,page.index],upper.bound) %>% max()
    y.plot.lower.bound <- c(varma.test[,page.index],lower.bound) %>% min()
    
    plot(1:nrow(varma.test),varma.test[,page.index],ylim=c(y.plot.upper.bound,y.plot.lower.bound))
    lines(1:nrow(varma.test),varma.test[,page.index]) # test data
    
    lines(1:nrow(varma.test),upper.bound,col="red",lwd=1) #predictions
    lines(1:nrow(varma.test),predictions,col="red",lwd=3)
    lines(1:nrow(varma.test),lower.bound,col="red",lwd=1)
    
    title(paste("VARMA(",varma.fit$ARorder,",",varma.fit$MAorder,") predictions on Page ",page.index,"/",ncol(varma.train),sep=""))
  
    observed.data <- varma.fit$data[2:nrow(varma.fit$data),page.index]
    residuals <- varma.fit$residuals[,page.index]
    
    SSE <- (residuals ** 2) %>% sum()
    TSS <- ((observed.data - mean(observed.data)) ** 2) %>% sum()
    
    varma.R.squared <- paste((1 - SSE/TSS) %>% as.character() %>% substr(3,4),"%",sep="")
    varma.AIC <- varma.fit$aic %>% as.character() %>% substr(0,4)
    varma.BIC <- varma.fit$bic %>% as.character() %>% substr(0,4)
    
    mtext(paste("R squared:",varma.R.squared,"AIC:",varma.AIC,"BIC:",varma.BIC))
    dev.off()
  }
}
```

# ARIMA

```{r arima, include=FALSE}
homepage.data <- web.traffic.data[which(web.traffic.data$Page=="www.usfca.edu/"),]
homepage.data <- homepage.data[order(homepage.data$Date),]
  
homepage.train <- homepage.data[1:(nrow(homepage.data)-14),"Pageviews"]
homepage.test <- homepage.data[(nrow(homepage.data)-13):(nrow(homepage.data)),"Pageviews"]

tryCatch({
  setwd("./rds")
  optimal.model <- readRDS("homepageArimaFit.rds")
},error=function(e){

  
  #fit for optimal parameters with d = 1
  min.aic <- 9999999999
  optimal.model <- NA
  for (p in 1:14) {
    for (q in 1:14) {
      tryCatch({
        arima.fit <- arima(homepage.train,order=c(p,1,q))
        if(arima.fit$aic < min.aic) {
          min.aic <- arima.fit$aic
          optimal.model <- arima.fit
        }
      },error=function(e){
        beep(7)
      })
    }
    beep(2)
  }
  beep(8)
  setwd("./rds")
  saveRDS(optimal.model,"homepageArimaFit.rds")
})

```

```{r plotARIMApredictions, include=FALSE}
homepage.predictions <- predict(optimal.model,newdata=homepage.test,n.ahead=14)

setwd("./img")
png("homePagePredictions.png")
plot(1:length(homepage.predictions$pred),homepage.test)
lines(1:length(homepage.predictions$pred),homepage.test)
lines(1:length(homepage.predictions$pred),homepage.predictions$pred,col="red")

p <- optimal.model$model$phi %>% length()
q <- optimal.model$model$theta %>% length()
d <- optimal.model$model$Delta
title(paste("ARIMA(",p,",",d,",",q,") predictions of www.usfca.edu/",sep=""))
dev.off()
```

```{r fitSARIMA, include=FALSE}
tryCatch({
  setwd("./rds")
  readRDS("monthlySarimaFit.rds")
},error=function(e){
  monthly.sarima.fit <- arima(homepage.train,seasonal=list(order=c(7,1,7),period=28))
  setwd("./rds")
  saveRDS(monthly.sarima.fit,"monthlySarimaFit.rds")
})
```

```{r finishedSuccessfully, include=FALSE}
beep(3)
```

TODO
Document performance of VARMA
Fit and document performance of ARIMA on USF's homepage
  Make ARIMA updatable