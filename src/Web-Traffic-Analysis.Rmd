---
title: "Web-Traffic-Analysis"
author: "Hume Dickie"
date: "November 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#The Goal

For my senior project, I wanted to familiarize myself with remote connections to databases from code in R & Python.

#The Data

The data for this project is large in size and scope. Via a connection to University of San Francisco's website through Google Analytics, many fields are available for about the last 12 months. So far, this project has used daily pageviews for each webpage.

The data has this format:
<table style="width:50%">
<tr>
<th>Page</th>
<th>7/27/2016</th>
<th>7/28/2016</th>
<th>...</th>
<th>10/23/2017</th>
</tr>
<tr>
<td>Page A</td>
<td align="center">0</td>
<td align="center">35</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr>
<td>Page B</td>
<td align="center">0</td>
<td align="center">50</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr>
<td>Page C</td>
<td align="center">0</td>
<td align="center">44</td>
<td align="center">40</td>
<td align="center">1</td>
</tr>
<tr>
<td>Page D</td>
<td align="center">0</td>
<td align="center">43</td>
<td align="center">40</td>
<td align="center">1</td>
</tr>
</table>

#Subsetting the Data

So far, I have focused on several subsets. The first subset is the webpages for the undergraduate majors, which is interesting to me as a student of the College of Arts & Sciences at USF. I also look at clusters of related pages.

One of the largest drivers of overall web traffic is the news articles on USF's website. In previous autoregression experiments, I ran into problems with sparse data. News articles and their parent webpages are analytically interesting since they have more significant and dynamic behavior.

#Models

I attempted to fit VAR (Vector Auto-Regressive) models to several subsets.

###Undergraduate Majors & Minors

Using VARSelect() from the 'vars' package, diagnostic plots indicate there was no possibility of obtaining useful predictions from this model. We would expect to see a non-zero minimum value to indicate an ideal number of lag variables, but the relentlessly increasing trend is a sign that increasing model complexity would only make predictions worse.

This model was fit on the 10 pages with the highest mean pageviews-per-day.

![](C:/Users/Hume Dickie/Desktop/Github/Senior-Project/img/VARselect-AIC-on-datset-3.png)

While disappointing, these results do indicate an absence of multi-day or weekly trends.

The situation is more promising on a collection of 4 related pages:

Page www.usfca.edu/arts-sciences/undergraduate-programs/biology

Page www.usfca.edu/arts-sciences/undergraduate-programs/biology/faculty

Page www.usfca.edu/arts-sciences/undergraduate-programs/biology/major-minor

Page www.usfca.edu/arts-sciences/undergraduate-programs/biology/pre-health

![](C:/Users/Hume Dickie/Desktop/Github/Senior-Project/img/related-pages-biology.png)

I expect to find a useful model with 7 lag variables.

###Top 10 News Articles

Fitting VAR on the 10 news articles with the highest median pageviews produced the following:

![](C:/Users/Hume Dickie/Desktop/Github/Senior-Project/img/top10news-median.png)

In this case, I would fit the model twice, once with 1 lag variable and again with 7 lag variables.

###New Questions

Can we relate the frequency of news posts to the activity? (More meaningfully than an average-pageviews per post?) Next, let's model post frequency and pageviews as poisson processes.

![](C:/Users/Hume Dickie/Desktop/Github/Senior-Project/img/news-posts-&-pageviews.png)

###To Be Continued...